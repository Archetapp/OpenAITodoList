{"primaryContentSections":[{"kind":"declarations","declarations":[{"platforms":["macOS"],"tokens":[{"text":"let","kind":"keyword"},{"text":" ","kind":"text"},{"text":"responseFormat","kind":"identifier"},{"text":": `Self`","kind":"text"},{"text":".","kind":"text"},{"kind":"typeIdentifier","preciseIdentifier":"s:6OpenAI9ChatQueryV14ResponseFormatO","text":"ResponseFormat","identifier":"doc:\/\/OpenAI\/documentation\/OpenAI\/ChatQuery\/ResponseFormat-swift.enum"},{"text":"?","kind":"text"}],"languages":["swift"]}]}],"hierarchy":{"paths":[["doc:\/\/OpenAI\/documentation\/OpenAI","doc:\/\/OpenAI\/documentation\/OpenAI\/ChatQuery"]]},"schemaVersion":{"major":0,"minor":3,"patch":0},"variants":[{"traits":[{"interfaceLanguage":"swift"}],"paths":["\/documentation\/openai\/chatquery\/responseformat-swift.property"]}],"kind":"symbol","sections":[],"identifier":{"interfaceLanguage":"swift","url":"doc:\/\/OpenAI\/documentation\/OpenAI\/ChatQuery\/responseFormat-swift.property"},"metadata":{"modules":[{"name":"OpenAI"}],"title":"responseFormat","roleHeading":"Instance Property","fragments":[{"kind":"keyword","text":"let"},{"kind":"text","text":" "},{"text":"responseFormat","kind":"identifier"},{"text":": `Self`","kind":"text"},{"text":".","kind":"text"},{"kind":"typeIdentifier","preciseIdentifier":"s:6OpenAI9ChatQueryV14ResponseFormatO","text":"ResponseFormat"},{"text":"?","kind":"text"}],"role":"symbol","externalID":"s:6OpenAI9ChatQueryV14responseFormatAC08ResponseF0OSgvp","symbolKind":"property"},"abstract":[{"text":"An object specifying the format that the model must output. Compatible with gpt-4-1106-preview and gpt-3.5-turbo-1106.","type":"text"},{"text":" ","type":"text"},{"text":"Setting to { “type”: “json_object” } enables JSON mode, which guarantees the message the model generates is valid JSON.","type":"text"},{"type":"text","text":" "},{"text":"Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly “stuck” request. Also note that the message content may be partially cut off if finish_reason=“length”, which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.","type":"text"}],"references":{"doc://OpenAI/documentation/OpenAI":{"url":"\/documentation\/openai","identifier":"doc:\/\/OpenAI\/documentation\/OpenAI","abstract":[],"type":"topic","role":"collection","kind":"symbol","title":"OpenAI"},"doc://OpenAI/documentation/OpenAI/ChatQuery/ResponseFormat-swift.enum":{"role":"symbol","fragments":[{"text":"enum","kind":"keyword"},{"kind":"text","text":" "},{"text":"ResponseFormat","kind":"identifier"}],"kind":"symbol","identifier":"doc:\/\/OpenAI\/documentation\/OpenAI\/ChatQuery\/ResponseFormat-swift.enum","navigatorTitle":[{"text":"ResponseFormat","kind":"identifier"}],"abstract":[],"url":"\/documentation\/openai\/chatquery\/responseformat-swift.enum","title":"ChatQuery.ResponseFormat","type":"topic"},"doc://OpenAI/documentation/OpenAI/ChatQuery/responseFormat-swift.property":{"kind":"symbol","title":"responseFormat","identifier":"doc:\/\/OpenAI\/documentation\/OpenAI\/ChatQuery\/responseFormat-swift.property","type":"topic","url":"\/documentation\/openai\/chatquery\/responseformat-swift.property","abstract":[{"text":"An object specifying the format that the model must output. Compatible with gpt-4-1106-preview and gpt-3.5-turbo-1106.","type":"text"},{"type":"text","text":" "},{"type":"text","text":"Setting to { “type”: “json_object” } enables JSON mode, which guarantees the message the model generates is valid JSON."},{"type":"text","text":" "},{"type":"text","text":"Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly “stuck” request. Also note that the message content may be partially cut off if finish_reason=“length”, which indicates the generation exceeded max_tokens or the conversation exceeded the max context length."}],"fragments":[{"text":"let","kind":"keyword"},{"kind":"text","text":" "},{"text":"responseFormat","kind":"identifier"},{"text":": `Self`","kind":"text"},{"text":".","kind":"text"},{"text":"ResponseFormat","preciseIdentifier":"s:6OpenAI9ChatQueryV14ResponseFormatO","kind":"typeIdentifier"},{"text":"?","kind":"text"}],"role":"symbol"},"doc://OpenAI/documentation/OpenAI/ChatQuery":{"role":"symbol","fragments":[{"kind":"keyword","text":"struct"},{"kind":"text","text":" "},{"text":"ChatQuery","kind":"identifier"}],"kind":"symbol","identifier":"doc:\/\/OpenAI\/documentation\/OpenAI\/ChatQuery","navigatorTitle":[{"text":"ChatQuery","kind":"identifier"}],"abstract":[{"text":"Creates a model response for the given chat conversation","type":"text"},{"text":" ","type":"text"},{"type":"text","text":"https:\/\/platform.openai.com\/docs\/guides\/text-generation"}],"url":"\/documentation\/openai\/chatquery","title":"ChatQuery","type":"topic"}}}